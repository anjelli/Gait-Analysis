{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install mediapipe"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "xhiRurtPeIe-",
        "outputId": "a35b1b91-a0ef-4a87-9b67-7730a218fd28"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mediapipe in /usr/local/lib/python3.11/dist-packages (0.10.21)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from mediapipe) (2.2.2)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (25.3.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (25.2.10)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.6.0)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.6.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from mediapipe) (3.10.1)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (1.26.4)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.11/dist-packages (from mediapipe) (4.11.0.86)\n",
            "Requirement already satisfied: protobuf<5,>=4.25.3 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (4.25.7)\n",
            "Requirement already satisfied: sounddevice>=0.4.4 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.5.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.2.0)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.11/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (0.5.1)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (1.15.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade numpy\n",
        "!pip install --upgrade pandas\n",
        "!pip install --upgrade tensorflow\n",
        "!pip install --upgrade mediapipe opencv-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ntIAVSVGed-Z",
        "outputId": "96e60c31-568a-4f96-9c25-a300017004ac"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Collecting numpy\n",
            "  Using cached numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "Using cached numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "mediapipe 0.10.21 requires numpy<2, but you have numpy 2.2.5 which is incompatible.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.2.5 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
            "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.19.0 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.5 which is incompatible.\n",
            "ydf 0.11.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 4.25.7 which is incompatible.\n",
            "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.19.0 which is incompatible.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.19.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.2.5\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.19.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.2.2)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Collecting numpy<2.2.0,>=1.26.0 (from tensorflow)\n",
            "  Using cached numpy-2.1.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.5.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.9)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Using cached numpy-2.1.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.3 MB)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.2.5\n",
            "    Uninstalling numpy-2.2.5:\n",
            "      Successfully uninstalled numpy-2.2.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "mediapipe 0.10.21 requires numpy<2, but you have numpy 2.1.3 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
            "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.19.0 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.1.3 which is incompatible.\n",
            "ydf 0.11.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 4.25.7 which is incompatible.\n",
            "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.19.0 which is incompatible.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.19.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.1.3\n",
            "Requirement already satisfied: mediapipe in /usr/local/lib/python3.11/dist-packages (0.10.21)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from mediapipe) (2.2.2)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (25.3.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (25.2.10)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.6.0)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.6.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from mediapipe) (3.10.1)\n",
            "Collecting numpy<2 (from mediapipe)\n",
            "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.11/dist-packages (from mediapipe) (4.11.0.86)\n",
            "Requirement already satisfied: protobuf<5,>=4.25.3 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (4.25.7)\n",
            "Requirement already satisfied: sounddevice>=0.4.4 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.5.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.2.0)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.11/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (0.5.1)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (1.15.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n",
            "Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.1.3\n",
            "    Uninstalling numpy-2.1.3:\n",
            "      Successfully uninstalled numpy-2.1.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
            "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.19.0 which is incompatible.\n",
            "ydf 0.11.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 4.25.7 which is incompatible.\n",
            "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.19.0 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.19.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "import time\n",
        "from collections import deque"
      ],
      "metadata": {
        "id": "2nZCJYvLdw1e"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the indices for relevant pose landmarks based on MediaPipe's model.\n",
        "# These are standard indices for the 33 landmarks provided by MediaPipe Pose.\n",
        "LEFT_HIP = 23\n",
        "RIGHT_HIP = 24\n",
        "LEFT_ANKLE = 27\n",
        "RIGHT_ANKLE = 28\n",
        "LEFT_HEEL = 29\n",
        "RIGHT_HEEL = 30\n",
        "LEFT_FOOT_INDEX = 31\n",
        "RIGHT_FOOT_INDEX = 32"
      ],
      "metadata": {
        "id": "NtG3A8Hjfqs0"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Class to store gait event information\n",
        "class GaitEvent:\n",
        "    def __init__(self, event_type, foot, timestamp, position):\n",
        "        self.event_type = event_type # 'Heel Strike' or 'Toe Off'\n",
        "        self.foot = foot # 'Left' or 'Right'\n",
        "        self.timestamp = timestamp # in seconds\n",
        "        self.position = position # Landmark position (x, y, z)"
      ],
      "metadata": {
        "id": "VxndEQWQfxhy"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Class to store gait cycle information\n",
        "class GaitCycle:\n",
        "    def __init__(self, foot, heel_strike_event, toe_off_event=None):\n",
        "        self.foot = foot\n",
        "        self.heel_strike = heel_strike_event\n",
        "        self.toe_off = toe_off_event\n",
        "        self.cycle_time = None\n",
        "        self.stride_length = None\n",
        "        self.stance_time = None\n",
        "        self.swing_time = None\n",
        "        self.double_support_times = [] # List of durations\n",
        "\n",
        "    def complete_cycle(self, next_heel_strike_time):\n",
        "        \"\"\"Calculates cycle time and stride length when the next heel strike occurs.\"\"\"\n",
        "        if self.heel_strike and next_heel_strike_time is not None:\n",
        "            self.cycle_time = next_heel_strike_time - self.heel_strike.timestamp\n",
        "\n",
        "    def complete_stance_swing(self, toe_off_event):\n",
        "        \"\"\"Calculates stance and swing time once toe off is detected.\"\"\"\n",
        "        self.toe_off = toe_off_event\n",
        "        if self.heel_strike and self.toe_off:\n",
        "            self.stance_time = self.toe_off.timestamp - self.heel_strike.timestamp\n",
        "            # Swing time is calculated when the next heel strike occurs (part of the next cycle's calculation)\n",
        "            # self.swing_time = next_heel_strike_time - self.toe_off.timestamp\n",
        "            pass"
      ],
      "metadata": {
        "id": "bB3lBipYf72-"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate Euclidean distance between two 3D points\n",
        "def calculate_distance(p1, p2):\n",
        "    \"\"\"Calculates the Euclidean distance between two points (x, y, z).\"\"\"\n",
        "    return np.sqrt((p1.x - p2.x)**2 + (p1.y - p2.y)**2 + (p1.z - p2.z)**2)\n",
        "\n",
        "# Function to detect gait events (Heel Strike and Toe Off)\n",
        "# This is a simplified example. Robust event detection requires careful algorithm design.\n",
        "def detect_gait_events(key_point_history, frame_rate):\n",
        "    \"\"\"\n",
        "    Detects gait events (Heel Strike and Toe Off) based on key point movement.\n",
        "\n",
        "    Args:\n",
        "        key_point_history: A deque of (timestamp, landmarks) tuples.\n",
        "        frame_rate: The frame rate of the video.\n",
        "\n",
        "    Returns:\n",
        "        A list of detected GaitEvent objects in the current frame.\n",
        "    \"\"\"\n",
        "    detected_events = []\n",
        "    if len(key_point_history) < 2:\n",
        "        return detected_events # Need at least two frames to detect movement\n",
        "\n",
        "    current_time, current_landmarks = key_point_history[-1]\n",
        "    previous_time, previous_landmarks = key_point_history[-2]\n",
        "\n",
        "    heel_strike_threshold_y = 0.005\n",
        "    toe_off_threshold_y = 0.005\n",
        "\n",
        "    # Check Left Foot\n",
        "    if len(current_landmarks) > max(LEFT_HEEL, LEFT_HIP) and len(previous_landmarks) > max(LEFT_HEEL, LEFT_HIP):\n",
        "        current_left_heel_y = current_landmarks[LEFT_HEEL].y\n",
        "        previous_left_heel_y = previous_landmarks[LEFT_HEEL].y\n",
        "        left_hip_y = current_landmarks[LEFT_HIP].y\n",
        "\n",
        "        # Simple Heel Strike Detection: Heel moves down significantly and is below hip\n",
        "        if current_left_heel_y > previous_left_heel_y + heel_strike_threshold_y and current_left_heel_y > left_hip_y:\n",
        "             # Add more robust checks here (e.g., velocity, relative position to ground)\n",
        "             detected_events.append(GaitEvent('Heel Strike', 'Left', current_time / 1000000.0, current_landmarks[LEFT_HEEL]))\n",
        "\n",
        "        # Simple Toe Off Detection: Foot index moves up significantly and is above hip\n",
        "        if len(current_landmarks) > max(LEFT_FOOT_INDEX, LEFT_HIP) and len(previous_landmarks) > max(LEFT_FOOT_INDEX, LEFT_HIP):\n",
        "            current_left_foot_index_y = current_landmarks[LEFT_FOOT_INDEX].y\n",
        "            previous_left_foot_index_y = previous_landmarks[LEFT_FOOT_INDEX].y\n",
        "            left_hip_y = current_landmarks[LEFT_HIP].y\n",
        "\n",
        "            if current_left_foot_index_y < previous_left_foot_index_y - toe_off_threshold_y and current_left_foot_index_y < left_hip_y:\n",
        "                 # Add more robust checks here\n",
        "                 detected_events.append(GaitEvent('Toe Off', 'Left', current_time / 1000000.0, current_landmarks[LEFT_FOOT_INDEX]))\n",
        "\n",
        "\n",
        "    # Check Right Foot (similar logic as Left Foot)\n",
        "    if len(current_landmarks) > max(RIGHT_HEEL, RIGHT_HIP) and len(previous_landmarks) > max(RIGHT_HEEL, RIGHT_HIP):\n",
        "        current_right_heel_y = current_landmarks[RIGHT_HEEL].y\n",
        "        previous_right_heel_y = previous_landmarks[RIGHT_HEEL].y\n",
        "        right_hip_y = current_landmarks[RIGHT_HIP].y\n",
        "\n",
        "        # Simple Heel Strike Detection\n",
        "        if current_right_heel_y > previous_right_heel_y + heel_strike_threshold_y and current_right_heel_y > right_hip_y:\n",
        "             detected_events.append(GaitEvent('Heel Strike', 'Right', current_time / 1000000.0, current_landmarks[RIGHT_HEEL]))\n",
        "\n",
        "        # Simple Toe Off Detection\n",
        "        if len(current_landmarks) > max(RIGHT_FOOT_INDEX, RIGHT_HIP) and len(previous_landmarks) > max(RIGHT_FOOT_INDEX, RIGHT_HIP):\n",
        "            current_right_foot_index_y = current_landmarks[RIGHT_FOOT_INDEX].y\n",
        "            previous_right_foot_index_y = previous_landmarks[RIGHT_FOOT_INDEX].y\n",
        "            right_hip_y = current_landmarks[RIGHT_HIP].y\n",
        "\n",
        "            if current_right_foot_index_y < previous_right_foot_index_y - toe_off_threshold_y and current_right_foot_index_y < right_hip_y:\n",
        "                 detected_events.append(GaitEvent('Toe Off', 'Right', current_time / 1000000.0, current_landmarks[RIGHT_FOOT_INDEX]))\n",
        "\n",
        "    return detected_events"
      ],
      "outputs": [],
      "execution_count": 107,
      "metadata": {
        "id": "dnsGHYOrdt8Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate gait metrics based on detected events and history\n",
        "def calculate_gait_metrics(key_point_history, detected_events, gait_cycles_left, gait_cycles_right, pixel_to_meter_ratio):\n",
        "    metrics = {}\n",
        "    current_time = key_point_history[-1][0] / 1000000.0\n",
        "    current_landmarks = key_point_history[-1][1]\n",
        "\n",
        "    # Update ongoing gait cycles and detect new ones\n",
        "    for event in detected_events:\n",
        "        if event.event_type == 'Heel Strike':\n",
        "            if event.foot == 'Left':\n",
        "                # Complete the previous left cycle if one was ongoing\n",
        "                if gait_cycles_left and gait_cycles_left[-1].toe_off:\n",
        "                     # Calculate swing time for the previous cycle\n",
        "                     gait_cycles_left[-1].swing_time = event.timestamp - gait_cycles_left[-1].toe_off.timestamp\n",
        "                     # For simplicity, we'll calculate cycle time here and leave stride length as a placeholder.\n",
        "                     if len(gait_cycles_left) > 1:\n",
        "                         gait_cycles_left[-2].complete_cycle(event.timestamp)\n",
        "\n",
        "                # Start a new left gait cycle\n",
        "                gait_cycles_left.append(GaitCycle('Left', event))\n",
        "\n",
        "            elif event.foot == 'Right':\n",
        "                 # Complete the previous right cycle if one was ongoing\n",
        "                 if gait_cycles_right and gait_cycles_right[-1].toe_off:\n",
        "                      gait_cycles_right[-1].swing_time = event.timestamp - gait_cycles_right[-1].toe_off.timestamp\n",
        "                      if len(gait_cycles_right) > 1:\n",
        "                          gait_cycles_right[-2].complete_cycle(event.timestamp)\n",
        "\n",
        "                 # Start a new right gait cycle\n",
        "                 gait_cycles_right.append(GaitCycle('Right', event))\n",
        "\n",
        "        elif event.event_type == 'Toe Off':\n",
        "            if event.foot == 'Left':\n",
        "                # Complete the stance phase of the current left cycle\n",
        "                if gait_cycles_left and gait_cycles_left[-1].heel_strike and gait_cycles_left[-1].toe_off is None:\n",
        "                    gait_cycles_left[-1].complete_stance_swing(event)\n",
        "            elif event.foot == 'Right':\n",
        "                 # Complete the stance phase of the current right cycle\n",
        "                 if gait_cycles_right and gait_cycles_right[-1].heel_strike and gait_cycles_right[-1].toe_off is None:\n",
        "                     gait_cycles_right[-1].complete_stance_swing(event)\n",
        "\n",
        "    # Calculate metrics from completed cycles (you might want to average over several cycles)\n",
        "    # Example: Calculate average cycle time from the last few completed cycles\n",
        "    num_cycles_to_average = 3 # This line and the following were incorrectly indented\n",
        "    left_cycle_times = [c.cycle_time for c in gait_cycles_left[-num_cycles_to_average:] if c.cycle_time is not None]\n",
        "    right_cycle_times = [c.cycle_time for c in gait_cycles_right[-num_cycles_to_average:] if c.cycle_time is not None]\n",
        "\n",
        "    if left_cycle_times:\n",
        "        metrics['average_left_cycle_time'] = np.mean(left_cycle_times)\n",
        "    if right_cycle_times:\n",
        "        metrics['average_right_cycle_time'] = np.mean(right_cycle_times)\n",
        "\n",
        "    # ... (rest of the function code remains the same)\n",
        "\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "A1B_uIU4geTf"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Main Pipeline Execution ---\n",
        "\n",
        "def run_gait_analysis_pipeline(video_path):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Error: Could not open video file {video_path}\")\n",
        "        return\n",
        "\n",
        "    frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
        "    if frame_rate == 0:\n",
        "        print(\"Warning: Could not get frame rate from video. Assuming 30 FPS.\")\n",
        "        frame_rate = 30.0\n",
        "\n",
        "    # Placeholder for pixel to meter ratio calibration\n",
        "    # You will need to determine this based on your video setup (e.g., using a known distance in the frame)\n",
        "    pixel_to_meter_ratio = 0.001 # Example: 1 pixel = 0.001 meters (adjust this!)\n",
        "\n",
        "    # Initialize MediaPipe Pose model\n",
        "    # 'static_image_mode=False' for video processing, 'model_complexity' can be 0, 1, or 2\n",
        "    with mp_pose.Pose(static_image_mode=False, model_complexity=1, enable_segmentation=False, min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
        "\n",
        "        key_point_history = deque(maxlen=int(frame_rate * 5)) # Store history for 5 seconds\n",
        "        gait_cycles_left = [] # To store completed left gait cycles\n",
        "        gait_cycles_right = [] # To store completed right gait cycles\n",
        "        ongoing_cycle_left = None # To track the current incomplete left cycle\n",
        "        ongoing_cycle_right = None # To track the current incomplete right cycle\n",
        "\n",
        "\n",
        "        frame_count = 0\n",
        "        start_time = time.time()\n",
        "\n",
        "        while cap.isOpened():\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            frame_count += 1\n",
        "            # Convert the BGR image to RGB.\n",
        "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            # Process the image and find pose landmarks.\n",
        "            results = pose.process(image)\n",
        "\n",
        "            # Convert the RGB image back to BGR for visualization.\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "            current_timestamp_us = int(cap.get(cv2.CAP_PROP_POS_MSEC) * 1000) # Timestamp in microseconds\n",
        "\n",
        "            if results.pose_landmarks:\n",
        "                landmarks = results.pose_landmarks.landmark\n",
        "                # Store the full set of landmarks in history\n",
        "                key_point_history.append((current_timestamp_us, landmarks))\n",
        "\n",
        "                # Extract relevant gait key points (hip, ankle, heel, foot index)\n",
        "                gait_landmarks = []\n",
        "                if len(landmarks) > max(RIGHT_FOOT_INDEX, LEFT_FOOT_INDEX): # Ensure enough landmarks are detected\n",
        "                    gait_landmarks.append(landmarks[LEFT_HIP])\n",
        "                    gait_landmarks.append(landmarks[RIGHT_HIP])\n",
        "                    gait_landmarks.append(landmarks[LEFT_ANKLE])\n",
        "                    gait_landmarks.append(landmarks[RIGHT_ANKLE])\n",
        "                    gait_landmarks.append(landmarks[LEFT_HEEL])\n",
        "                    gait_landmarks.append(landmarks[RIGHT_HEEL])\n",
        "                    gait_landmarks.append(landmarks[LEFT_FOOT_INDEX])\n",
        "                    gait_landmarks.append(landmarks[RIGHT_FOOT_INDEX])\n",
        "\n",
        "                # Detect gait events in the current frame\n",
        "                detected_events = detect_gait_events(key_point_history, frame_rate)\n",
        "\n",
        "                # Calculate gait metrics\n",
        "                # Pass ongoing cycles to the calculation function to update them\n",
        "                gait_metrics = calculate_gait_metrics(key_point_history, detected_events, gait_cycles_left, gait_cycles_right, pixel_to_meter_ratio)\n",
        "\n",
        "                # --- Output or Display Metrics ---\n",
        "                # You can print the metrics, display them on the frame, or save them to a file.\n",
        "                print(f\"Frame: {frame_count}, Timestamp: {current_timestamp_us / 1000000.0:.2f}s, Metrics: {gait_metrics}\")\n",
        "\n",
        "                # Optional: Draw landmarks on the frame\n",
        "                mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
        "                                          mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=2),\n",
        "                                          mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2))\n",
        "\n",
        "            # Display the frame\n",
        "            cv2.imshow('MediaPipe Pose and Gait Analysis', image)\n",
        "\n",
        "            # Break the loop if 'q' is pressed\n",
        "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "                break\n",
        "\n",
        "        end_time = time.time()\n",
        "        elapsed_time = end_time - start_time\n",
        "        print(f\"\\nProcessing finished.\")\n",
        "        print(f\"Processed {frame_count} frames in {elapsed_time:.2f} seconds.\")\n",
        "        if elapsed_time > 0:\n",
        "            print(f\"Average FPS: {frame_count / elapsed_time:.2f}\")\n",
        "\n",
        "\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "fhT5TB2yglas"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "video_path = \"output_video_test_high_.mp4\"\n",
        "run_gait_analysis_pipeline(video_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "9Br9U6SljCi9",
        "outputId": "b04922a9-3fbe-4cd1-b3ad-1fa951bd1457"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ade08d58-a68d-48cc-9d0e-d17cc89a5069\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ade08d58-a68d-48cc-9d0e-d17cc89a5069\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving output_video_test_high_.mp4 to output_video_test_high_ (2).mp4\n",
            "Error: Could not open video file output_video_test_high_.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "video_path_ = \"output_video_test_high_ (2).mp4\""
      ],
      "metadata": {
        "id": "ZgqAW--GYdEi"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_gait_analysis_pipeline(video_path_):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Failed to open video: {video_path_}\")\n",
        "        return\n",
        "\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    frame_count = 0\n",
        "\n",
        "    left_ankle_y, right_ankle_y = [], []\n",
        "    left_ankle_x, right_ankle_x = [], []\n",
        "    left_hip_y, right_hip_y = [], []\n",
        "    timestamps = []\n",
        "\n",
        "    pose_detected_frames = 0\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        results = pose.process(image_rgb)\n",
        "\n",
        "        if results.pose_landmarks:\n",
        "            pose_detected_frames += 1\n",
        "            landmarks = results.pose_landmarks.landmark\n",
        "\n",
        "            l_ankle = landmarks[mp_pose.PoseLandmark.LEFT_ANKLE]\n",
        "            r_ankle = landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE]\n",
        "            l_hip = landmarks[mp_pose.PoseLandmark.LEFT_HIP]\n",
        "            r_hip = landmarks[mp_pose.PoseLandmark.RIGHT_HIP]\n",
        "\n",
        "            left_ankle_y.append(l_ankle.y)\n",
        "            right_ankle_y.append(r_ankle.y)\n",
        "            left_ankle_x.append(l_ankle.x)\n",
        "            right_ankle_x.append(r_ankle.x)\n",
        "            left_hip_y.append(l_hip.y)\n",
        "            right_hip_y.append(r_hip.y)\n",
        "            timestamps.append(frame_count / fps)\n",
        "\n",
        "        frame_count += 1\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    print(f\"Total frames processed: {frame_count}\")\n",
        "    print(f\"Frames with pose detected: {pose_detected_frames}\")\n",
        "\n",
        "    compute_gait_metrics(\n",
        "        left_ankle_y, right_ankle_y, left_ankle_x, right_ankle_x, timestamps, fps\n",
        "    )\n"
      ],
      "metadata": {
        "id": "cLDQeO2eq_Du"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.signal import find_peaks, savgol_filter\n",
        "\n",
        "# Initialize MediaPipe Pose\n",
        "mp_pose = mp.solutions.pose\n",
        "pose = mp_pose.Pose()\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "\n",
        "def run_gait_analysis_pipeline(video_path_):\n",
        "    cap = cv2.VideoCapture(video_path_)\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    frame_count = 0\n",
        "\n",
        "    left_ankle_y, right_ankle_y = [], []\n",
        "    left_ankle_x, right_ankle_x = [], []\n",
        "    left_hip_y, right_hip_y = [], []\n",
        "    timestamps = []\n",
        "\n",
        "    pose_detected_frames = 0\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        results = pose.process(image_rgb)\n",
        "\n",
        "        if results.pose_landmarks:\n",
        "            pose_detected_frames += 1\n",
        "            landmarks = results.pose_landmarks.landmark\n",
        "\n",
        "            l_ankle = landmarks[mp_pose.PoseLandmark.LEFT_ANKLE]\n",
        "            r_ankle = landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE]\n",
        "            l_hip = landmarks[mp_pose.PoseLandmark.LEFT_HIP]\n",
        "            r_hip = landmarks[mp_pose.PoseLandmark.RIGHT_HIP]\n",
        "\n",
        "            left_ankle_y.append(l_ankle.y)\n",
        "            right_ankle_y.append(r_ankle.y)\n",
        "            left_ankle_x.append(l_ankle.x)\n",
        "            right_ankle_x.append(r_ankle.x)\n",
        "            left_hip_y.append(l_hip.y)\n",
        "            right_hip_y.append(r_hip.y)\n",
        "            timestamps.append(frame_count / fps)\n",
        "\n",
        "        frame_count += 1\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    print(f\"Total frames processed: {frame_count}\")\n",
        "    print(f\"Frames with pose detected: {pose_detected_frames}\")\n",
        "\n",
        "    compute_gait_metrics(\n",
        "        left_ankle_y, right_ankle_y, left_ankle_x, right_ankle_x, timestamps, fps\n",
        "    )\n",
        "    cap = cv2.VideoCapture(video_path_)\n",
        "    if not cap.isOpened():\n",
        "        print(f\" Failed to open video: {video_path_}\")\n",
        "        return\n",
        "    else:\n",
        "        print(f\"Successfully opened video: {video_path_}\")\n",
        "    video_path_ = \"/content/output_video_test_high_ (2).mp4\"\n",
        "    result = run_gait_analysis_pipeline(video_path)\n",
        "    print(result)\n",
        "\n",
        "def compute_gait_metrics(lay, ray, lax, rax, timestamps, fps):\n",
        "    if not lay or not ray or not lax or not rax or not timestamps:\n",
        "        print(\"Insufficient data for gait metrics.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Data points available: {len(lay)} frames\")\n",
        "\n",
        "    signal = -np.array(lay)\n",
        "    min_peak_distance = max(1, int(fps * 0.3))\n",
        "\n",
        "    peaks, _ = find_peaks(signal, distance=min_peak_distance)\n",
        "    print(f\"Detected {len(peaks)} gait peaks\")\n",
        "\n",
        "    if len(peaks) < 2:\n",
        "        print(\"Not enough peaks for cycle computation.\")\n",
        "        return\n",
        "\n",
        "    cycle_times = np.diff([timestamps[p] for p in peaks])\n",
        "    avg_cycle_time = np.mean(cycle_times) if len(cycle_times) > 1 else None\n",
        "\n",
        "    step_lengths = [abs(lax[i] - rax[i]) for i in peaks if i < len(lax) and i < len(rax)]\n",
        "    avg_step_length = np.mean(step_lengths) if step_lengths else None\n",
        "\n",
        "    stride_lengths = [\n",
        "        abs(lax[peaks[i]] - lax[peaks[i - 1]])\n",
        "        for i in range(1, len(peaks))\n",
        "        if peaks[i] < len(lax) and peaks[i - 1] < len(lax)\n",
        "    ]\n",
        "    avg_stride_length = np.mean(stride_lengths) if stride_lengths else None\n",
        "\n",
        "    gait_speed = (\n",
        "        avg_stride_length / avg_cycle_time\n",
        "        if avg_stride_length is not None and avg_cycle_time\n",
        "        else None\n",
        "    )\n",
        "\n",
        "    stance_l = estimate_stance_times(lay, timestamps)\n",
        "    stance_r = estimate_stance_times(ray, timestamps)\n",
        "\n",
        "    avg_stance_l = np.mean([end - start for start, end in stance_l]) if stance_l else None\n",
        "    avg_stance_r = np.mean([end - start for start, end in stance_r]) if stance_r else None\n",
        "\n",
        "    double_supports = 0\n",
        "    for ls in stance_l:\n",
        "        for rs in stance_r:\n",
        "            overlap = max(0, min(ls[1], rs[1]) - max(ls[0], rs[0]))\n",
        "            if overlap > 0:\n",
        "                double_supports += overlap\n",
        "    avg_double_support = double_supports / min(len(stance_l), len(stance_r)) if stance_l and stance_r else None\n",
        "\n",
        "    print(\"\\n=== Gait Metrics ===\")\n",
        "    print(f\"Average Cycle Time:       {avg_cycle_time:.2f} s\" if avg_cycle_time is not None else \"Average Cycle Time:       N/A\")\n",
        "    print(f\"Average Step Length:      {avg_step_length:.3f} units\" if avg_step_length is not None else \"Average Step Length:      N/A\")\n",
        "    print(f\"Average Stride Length:    {avg_stride_length:.3f} units\" if avg_stride_length is not None else \"Average Stride Length:    N/A\")\n",
        "    print(f\"Gait Speed:               {gait_speed:.3f} units/s\" if gait_speed is not None else \"Gait Speed:               N/A\")\n",
        "    print(f\"Average Stance Time (L):  {avg_stance_l:.2f} s\" if avg_stance_l is not None else \"Average Stance Time (L):  N/A\")\n",
        "    print(f\"Average Stance Time (R):  {avg_stance_r:.2f} s\" if avg_stance_r is not None else \"Average Stance Time (R):  N/A\")\n",
        "    print(f\"Average Double Support:   {avg_double_support:.2f} s\" if avg_double_support is not None else \"Average Double Support:   N/A\")\n",
        "\n",
        "def estimate_stance_times(y_coords, timestamps, vel_thresh=0.015, min_stance_dur=0.05):\n",
        "    y = np.array(y_coords)\n",
        "    if len(y) < 9:\n",
        "        return []\n",
        "\n",
        "    # Apply Savitzky-Golay smoothing to remove noise\n",
        "    y_smooth = savgol_filter(y, window_length=9, polyorder=2)\n",
        "    timestamps = np.array(timestamps)\n",
        "\n",
        "    dt = np.diff(timestamps)\n",
        "    dy = np.diff(y_smooth)\n",
        "    velocity = np.abs(dy / dt)\n",
        "\n",
        "    stance_periods = []\n",
        "    in_stance = False\n",
        "    start_time = None\n",
        "\n",
        "    for i, v in enumerate(velocity):\n",
        "        if v < vel_thresh:\n",
        "            if not in_stance:\n",
        "                in_stance = True\n",
        "                start_time = timestamps[i]\n",
        "        else:\n",
        "            if in_stance:\n",
        "                end_time = timestamps[i]\n",
        "                duration = end_time - start_time\n",
        "                if duration >= min_stance_dur:\n",
        "                    stance_periods.append((start_time, end_time))\n",
        "                in_stance = False\n",
        "\n",
        "    return stance_periods\n",
        "\n",
        "run_gait_analysis_pipeline(video_path_)"
      ],
      "metadata": {
        "id": "-7Nn71INuZC0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bab31ffa-34a5-4b43-fe06-55ed1dce2c5c"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total frames processed: 527\n",
            "Frames with pose detected: 300\n",
            "Data points available: 300 frames\n",
            "Detected 11 gait peaks\n",
            "\n",
            "=== Gait Metrics ===\n",
            "Average Cycle Time:       0.45 s\n",
            "Average Step Length:      0.033 units\n",
            "Average Stride Length:    0.048 units\n",
            "Gait Speed:               0.106 units/s\n",
            "Average Stance Time (L):  0.40 s\n",
            "Average Stance Time (R):  0.38 s\n",
            "Average Double Support:   0.33 s\n",
            "Successfully opened video: output_video_test_high_ (2).mp4\n",
            "Total frames processed: 0\n",
            "Frames with pose detected: 0\n",
            "Insufficient data for gait metrics.\n",
            " Failed to open video: /content/output_video_test_high_(2).mp4\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "\n",
        "# Initialize MediaPipe Pose\n",
        "mp_pose = mp.solutions.pose\n",
        "pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "\n",
        "# Define connections for stick figure\n",
        "POSE_CONNECTIONS = [\n",
        "    (mp_pose.PoseLandmark.LEFT_HIP, mp_pose.PoseLandmark.LEFT_KNEE),\n",
        "    (mp_pose.PoseLandmark.LEFT_KNEE, mp_pose.PoseLandmark.LEFT_ANKLE),\n",
        "    (mp_pose.PoseLandmark.RIGHT_HIP, mp_pose.PoseLandmark.RIGHT_KNEE),\n",
        "    (mp_pose.PoseLandmark.RIGHT_KNEE, mp_pose.PoseLandmark.RIGHT_ANKLE),\n",
        "    (mp_pose.PoseLandmark.LEFT_HIP, mp_pose.PoseLandmark.RIGHT_HIP),\n",
        "    (mp_pose.PoseLandmark.LEFT_SHOULDER, mp_pose.PoseLandmark.RIGHT_SHOULDER),\n",
        "    (mp_pose.PoseLandmark.LEFT_SHOULDER, mp_pose.PoseLandmark.LEFT_HIP),\n",
        "    (mp_pose.PoseLandmark.RIGHT_SHOULDER, mp_pose.PoseLandmark.RIGHT_HIP),\n",
        "    (mp_pose.PoseLandmark.LEFT_SHOULDER, mp_pose.PoseLandmark.LEFT_ELBOW),\n",
        "    (mp_pose.PoseLandmark.LEFT_ELBOW, mp_pose.PoseLandmark.LEFT_WRIST),\n",
        "    (mp_pose.PoseLandmark.RIGHT_SHOULDER, mp_pose.PoseLandmark.RIGHT_ELBOW),\n",
        "    (mp_pose.PoseLandmark.RIGHT_ELBOW, mp_pose.PoseLandmark.RIGHT_WRIST),\n",
        "    (mp_pose.PoseLandmark.NOSE, mp_pose.PoseLandmark.LEFT_EYE),\n",
        "    (mp_pose.PoseLandmark.NOSE, mp_pose.PoseLandmark.RIGHT_EYE),\n",
        "]\n",
        "\n",
        "def draw_stick_figure(image, landmarks, connections, visibility_th=0.5):\n",
        "    h, w, _ = image.shape\n",
        "    for connection in connections:\n",
        "        start_idx, end_idx = connection\n",
        "        start = landmarks[start_idx.value]\n",
        "        end = landmarks[end_idx.value]\n",
        "        if start.visibility > visibility_th and end.visibility > visibility_th:\n",
        "            x1, y1 = int(start.x * w), int(start.y * h)\n",
        "            x2, y2 = int(end.x * w), int(end.y * h)\n",
        "            cv2.line(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "    return image\n",
        "\n",
        "def visualize_gait(video_path, output_path='gait_output.mp4'):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Failed to open video: {video_path}\")\n",
        "        return\n",
        "\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "    frame_count = 0\n",
        "    pose_detected_frames = 0\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        results = pose.process(image_rgb)\n",
        "\n",
        "        if results.pose_landmarks:\n",
        "            pose_detected_frames += 1\n",
        "            frame = draw_stick_figure(frame, results.pose_landmarks.landmark, POSE_CONNECTIONS)\n",
        "\n",
        "        out.write(frame)\n",
        "        frame_count += 1\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    print(f\"Total frames processed: {frame_count}\")\n",
        "    print(f\"Frames with pose detected: {pose_detected_frames}\")\n",
        "    print(f\"Output video saved to: {output_path}\")\n"
      ],
      "metadata": {
        "id": "usB1WDuog5js"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "import time\n",
        "from collections import deque\n",
        "from scipy.signal import find_peaks, savgol_filter\n",
        "import os\n",
        "from google.colab import files # Import the files module for downloading\n",
        "\n",
        "# --- Initialization ---\n",
        "# Initialize MediaPipe Pose model\n",
        "# static_image_mode=False for video processing\n",
        "# model_complexity can be 0, 1, or 2 (higher is more accurate but slower)\n",
        "# min_detection_confidence and min_tracking_confidence are thresholds for detection and tracking\n",
        "mp_pose = mp.solutions.pose\n",
        "pose = mp_pose.Pose(static_image_mode=False, model_complexity=1, enable_segmentation=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "\n",
        "# Define the indices for relevant pose landmarks based on MediaPipe's model.\n",
        "# These are standard indices for the 33 landmarks provided by MediaPipe Pose.\n",
        "LEFT_HIP = 23\n",
        "RIGHT_HIP = 24\n",
        "LEFT_ANKLE = 27\n",
        "RIGHT_ANKLE = 28\n",
        "LEFT_HEEL = 29\n",
        "RIGHT_HEEL = 30\n",
        "LEFT_FOOT_INDEX = 31\n",
        "RIGHT_FOOT_INDEX = 32\n",
        "\n",
        "# Define connections for stick figure visualization\n",
        "POSE_CONNECTIONS = [\n",
        "    (mp_pose.PoseLandmark.LEFT_HIP, mp_pose.PoseLandmark.LEFT_KNEE),\n",
        "    (mp_pose.PoseLandmark.LEFT_KNEE, mp_pose.PoseLandmark.LEFT_ANKLE),\n",
        "    (mp_pose.PoseLandmark.RIGHT_HIP, mp_pose.PoseLandmark.RIGHT_KNEE),\n",
        "    (mp_pose.PoseLandmark.RIGHT_KNEE, mp_pose.PoseLandmark.RIGHT_ANKLE),\n",
        "    (mp_pose.PoseLandmark.LEFT_HIP, mp_pose.PoseLandmark.RIGHT_HIP),\n",
        "    (mp_pose.PoseLandmark.LEFT_SHOULDER, mp_pose.PoseLandmark.RIGHT_SHOULDER),\n",
        "    (mp_pose.PoseLandmark.LEFT_SHOULDER, mp_pose.PoseLandmark.LEFT_HIP),\n",
        "    (mp_pose.PoseLandmark.RIGHT_SHOULDER, mp_pose.PoseLandmark.RIGHT_HIP),\n",
        "    (mp_pose.PoseLandmark.LEFT_SHOULDER, mp_pose.PoseLandmark.LEFT_ELBOW),\n",
        "    (mp_pose.PoseLandmark.LEFT_ELBOW, mp_pose.PoseLandmark.LEFT_WRIST),\n",
        "    (mp_pose.PoseLandmark.RIGHT_SHOULDER, mp_pose.PoseLandmark.RIGHT_ELBOW),\n",
        "    (mp_pose.PoseLandmark.RIGHT_ELBOW, mp_pose.PoseLandmark.RIGHT_WRIST),\n",
        "    (mp_pose.PoseLandmark.NOSE, mp_pose.PoseLandmark.LEFT_EYE),\n",
        "    (mp_pose.PoseLandmark.NOSE, mp_pose.PoseLandmark.RIGHT_EYE),\n",
        "]\n",
        "\n",
        "# --- Data Structures for Gait Analysis ---\n",
        "# Class to store gait event information\n",
        "class GaitEvent:\n",
        "    def __init__(self, event_type, foot, timestamp, position):\n",
        "        self.event_type = event_type # 'Heel Strike' or 'Toe Off'\n",
        "        self.foot = foot # 'Left' or 'Right'\n",
        "        self.timestamp = timestamp # in seconds\n",
        "        self.position = position # Landmark position (x, y, z)\n",
        "\n",
        "# Class to store gait cycle information\n",
        "class GaitCycle:\n",
        "    def __init__(self, foot, heel_strike_event, toe_off_event=None):\n",
        "        self.foot = foot\n",
        "        self.heel_strike = heel_strike_event\n",
        "        self.toe_off = toe_off_event\n",
        "        self.cycle_time = None\n",
        "        self.stride_length = None # Placeholder, requires calibration\n",
        "        self.stance_time = None\n",
        "        self.swing_time = None\n",
        "        self.double_support_times = [] # List of durations\n",
        "\n",
        "    def complete_cycle(self, next_heel_strike_time):\n",
        "        \"\"\"Calculates cycle time when the next heel strike occurs.\"\"\"\n",
        "        if self.heel_strike and next_heel_strike_time is not None:\n",
        "            self.cycle_time = next_heel_strike_time - self.heel_strike.timestamp\n",
        "\n",
        "    def complete_stance_swing(self, toe_off_event):\n",
        "        \"\"\"Calculates stance time once toe off is detected.\"\"\"\n",
        "        self.toe_off = toe_off_event\n",
        "        if self.heel_strike and self.toe_off:\n",
        "            self.stance_time = self.toe_off.timestamp - self.heel_strike.timestamp\n",
        "            # Swing time is calculated when the next heel strike occurs (part of the next cycle's calculation)\n",
        "            pass\n",
        "\n",
        "# --- Helper Functions ---\n",
        "def calculate_distance(p1, p2):\n",
        "    \"\"\"Calculates the Euclidean distance between two points (x, y, z).\"\"\"\n",
        "    # Ensure points have x, y, z attributes\n",
        "    if not hasattr(p1, 'x') or not hasattr(p1, 'y') or not hasattr(p1, 'z') or \\\n",
        "       not hasattr(p2, 'x') or not hasattr(p2, 'y') or not hasattr(p2, 'z'):\n",
        "        print(\"Warning: Points do not have valid coordinates for distance calculation.\")\n",
        "        return 0.0\n",
        "    return np.sqrt((p1.x - p2.x)**2 + (p1.y - p2.y)**2 + (p1.z - p2.z)**2)\n",
        "\n",
        "def detect_gait_events(key_point_history, frame_rate):\n",
        "    \"\"\"\n",
        "    Detects gait events (Heel Strike and Toe Off) based on key point movement.\n",
        "    This is a simplified example. Robust event detection requires careful algorithm design.\n",
        "\n",
        "    Args:\n",
        "        key_point_history: A deque of (timestamp, landmarks) tuples.\n",
        "        frame_rate: The frame rate of the video.\n",
        "\n",
        "    Returns:\n",
        "        A list of detected GaitEvent objects in the current frame.\n",
        "    \"\"\"\n",
        "    detected_events = []\n",
        "    if len(key_point_history) < 2:\n",
        "        return detected_events # Need at least two frames to detect movement\n",
        "\n",
        "    current_time, current_landmarks = key_point_history[-1]\n",
        "    previous_time, previous_landmarks = key_point_history[-2]\n",
        "\n",
        "    # Thresholds for detecting significant vertical movement\n",
        "    # These thresholds may need tuning based on video resolution and subject's movement\n",
        "    heel_strike_threshold_y = 0.005\n",
        "    toe_off_threshold_y = 0.005\n",
        "\n",
        "    # Ensure landmarks are detected for relevant points\n",
        "    if current_landmarks is None or previous_landmarks is None:\n",
        "        return detected_events\n",
        "\n",
        "    # Check Left Foot\n",
        "    if len(current_landmarks) > max(LEFT_HEEL, LEFT_HIP) and len(previous_landmarks) > max(LEFT_HEEL, LEFT_HIP):\n",
        "        current_left_heel_y = current_landmarks[LEFT_HEEL].y\n",
        "        previous_left_heel_y = previous_landmarks[LEFT_HEEL].y\n",
        "        left_hip_y = current_landmarks[LEFT_HIP].y\n",
        "\n",
        "        # Simple Heel Strike Detection: Heel moves down significantly and is below hip (relative y-coordinate)\n",
        "        # This is a basic heuristic and can be improved.\n",
        "        if current_left_heel_y > previous_left_heel_y + heel_strike_threshold_y and current_left_heel_y > left_hip_y:\n",
        "             # Add more robust checks here (e.g., velocity, relative position to ground)\n",
        "             detected_events.append(GaitEvent('Heel Strike', 'Left', current_time / 1000000.0, current_landmarks[LEFT_HEEL]))\n",
        "\n",
        "        # Simple Toe Off Detection: Foot index moves up significantly and is above hip (relative y-coordinate)\n",
        "        # This is a basic heuristic and can be improved.\n",
        "        if len(current_landmarks) > max(LEFT_FOOT_INDEX, LEFT_HIP) and len(previous_landmarks) > max(LEFT_FOOT_INDEX, LEFT_HIP):\n",
        "            current_left_foot_index_y = current_landmarks[LEFT_FOOT_INDEX].y\n",
        "            previous_left_foot_index_y = previous_landmarks[LEFT_FOOT_INDEX].y\n",
        "            left_hip_y = current_landmarks[LEFT_HIP].y\n",
        "\n",
        "            if current_left_foot_index_y < previous_left_foot_index_y - toe_off_threshold_y and current_left_foot_index_y < left_hip_y:\n",
        "                 # Add more robust checks here\n",
        "                 detected_events.append(GaitEvent('Toe Off', 'Left', current_time / 1000000.0, current_landmarks[LEFT_FOOT_INDEX]))\n",
        "\n",
        "    # Check Right Foot (similar logic as Left Foot)\n",
        "    if len(current_landmarks) > max(RIGHT_HEEL, RIGHT_HIP) and len(previous_landmarks) > max(RIGHT_HEEL, RIGHT_HIP):\n",
        "        current_right_heel_y = current_landmarks[RIGHT_HEEL].y\n",
        "        previous_right_heel_y = previous_landmarks[RIGHT_HEEL].y\n",
        "        right_hip_y = current_landmarks[RIGHT_HIP].y\n",
        "\n",
        "        # Simple Heel Strike Detection\n",
        "        if current_right_heel_y > previous_right_heel_y + heel_strike_threshold_y and current_right_heel_y > right_hip_y:\n",
        "             detected_events.append(GaitEvent('Heel Strike', 'Right', current_time / 1000000.0, current_landmarks[RIGHT_HEEL]))\n",
        "\n",
        "        # Simple Toe Off Detection\n",
        "        if len(current_landmarks) > max(RIGHT_FOOT_INDEX, RIGHT_HIP) and len(previous_landmarks) > max(RIGHT_FOOT_INDEX, RIGHT_HIP):\n",
        "            current_right_foot_index_y = current_landmarks[RIGHT_FOOT_INDEX].y\n",
        "            previous_right_foot_index_y = previous_landmarks[RIGHT_FOOT_INDEX].y\n",
        "            right_hip_y = current_landmarks[RIGHT_HIP].y\n",
        "\n",
        "            if current_right_foot_index_y < previous_right_foot_index_y - toe_off_threshold_y and current_right_foot_index_y < right_hip_y:\n",
        "                 detected_events.append(GaitEvent('Toe Off', 'Right', current_time / 1000000.0, current_landmarks[RIGHT_FOOT_INDEX]))\n",
        "\n",
        "    return detected_events\n",
        "\n",
        "def calculate_gait_metrics(key_point_history, detected_events, gait_cycles_left, gait_cycles_right, pixel_to_meter_ratio):\n",
        "    \"\"\"\n",
        "    Calculates gait metrics based on detected events and landmark history.\n",
        "\n",
        "    Args:\n",
        "        key_point_history: A deque of (timestamp, landmarks) tuples.\n",
        "        detected_events: A list of GaitEvent objects detected in the current frame.\n",
        "        gait_cycles_left: List of completed left gait cycles.\n",
        "        gait_cycles_right: List of completed right gait cycles.\n",
        "        pixel_to_meter_ratio: Conversion factor from pixels to meters (requires calibration).\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing calculated gait metrics for the current frame/state.\n",
        "    \"\"\"\n",
        "    metrics = {}\n",
        "    current_time = key_point_history[-1][0] / 1000000.0 if key_point_history else 0\n",
        "    current_landmarks = key_point_history[-1][1] if key_point_history else None\n",
        "\n",
        "    if current_landmarks is None:\n",
        "        return metrics # Return empty metrics if no landmarks are detected\n",
        "\n",
        "    # Update ongoing gait cycles and detect new ones\n",
        "    for event in detected_events:\n",
        "        if event.event_type == 'Heel Strike':\n",
        "            if event.foot == 'Left':\n",
        "                # Complete the previous left cycle if one was ongoing and had a toe off\n",
        "                if gait_cycles_left and gait_cycles_left[-1].toe_off:\n",
        "                     gait_cycles_left[-1].swing_time = event.timestamp - gait_cycles_left[-1].toe_off.timestamp\n",
        "                     # Complete the cycle time for the second to last cycle (Heel Strike to next Heel Strike)\n",
        "                     if len(gait_cycles_left) > 1:\n",
        "                         gait_cycles_left[-2].complete_cycle(event.timestamp)\n",
        "\n",
        "                # Start a new left gait cycle\n",
        "                gait_cycles_left.append(GaitCycle('Left', event))\n",
        "\n",
        "            elif event.foot == 'Right':\n",
        "                 # Complete the previous right cycle if one was ongoing and had a toe off\n",
        "                 if gait_cycles_right and gait_cycles_right[-1].toe_off:\n",
        "                      gait_cycles_right[-1].swing_time = event.timestamp - gait_cycles_right[-1].toe_off.timestamp\n",
        "                      # Complete the cycle time for the second to last cycle\n",
        "                      if len(gait_cycles_right) > 1:\n",
        "                          gait_cycles_right[-2].complete_cycle(event.timestamp)\n",
        "\n",
        "                 # Start a new right gait cycle\n",
        "                 gait_cycles_right.append(GaitCycle('Right', event))\n",
        "\n",
        "        elif event.event_type == 'Toe Off':\n",
        "            if event.foot == 'Left':\n",
        "                # Complete the stance phase of the current left cycle if it's ongoing\n",
        "                if gait_cycles_left and gait_cycles_left[-1].heel_strike and gait_cycles_left[-1].toe_off is None:\n",
        "                    gait_cycles_left[-1].complete_stance_swing(event)\n",
        "            elif event.foot == 'Right':\n",
        "                 # Complete the stance phase of the current right cycle if it's ongoing\n",
        "                 if gait_cycles_right and gait_cycles_right[-1].heel_strike and gait_cycles_right[-1].toe_off is None:\n",
        "                     gait_cycles_right[-1].complete_stance_swing(event)\n",
        "\n",
        "    # Calculate metrics from completed cycles (you might want to average over several cycles)\n",
        "    # Example: Calculate average cycle time from the last few completed cycles\n",
        "    num_cycles_to_average = 3\n",
        "    left_cycle_times = [c.cycle_time for c in gait_cycles_left[-num_cycles_to_average:] if c.cycle_time is not None]\n",
        "    right_cycle_times = [c.cycle_time for c in gait_cycles_right[-num_cycles_to_average:] if c.cycle_time is not None]\n",
        "\n",
        "    if left_cycle_times:\n",
        "        metrics['average_left_cycle_time'] = np.mean(left_cycle_times)\n",
        "    if right_cycle_times:\n",
        "        metrics['average_right_cycle_time'] = np.mean(right_cycle_times)\n",
        "\n",
        "    # Calculate average stance and swing times from completed cycles\n",
        "    left_stance_times = [c.stance_time for c in gait_cycles_left[-num_cycles_to_average:] if c.stance_time is not None]\n",
        "    right_stance_times = [c.stance_time for c in gait_cycles_right[-num_cycles_to_average:] if c.stance_time is not None]\n",
        "    left_swing_times = [c.swing_time for c in gait_cycles_left[-num_cycles_to_average:] if c.swing_time is not None]\n",
        "    right_swing_times = [c.swing_time for c in gait_cycles_right[-num_cycles_to_average:] if c.swing_time is not None]\n",
        "\n",
        "\n",
        "    if left_stance_times:\n",
        "        metrics['average_left_stance_time'] = np.mean(left_stance_times)\n",
        "    if right_stance_times:\n",
        "        metrics['average_right_stance_time'] = np.mean(right_stance_times)\n",
        "    if left_swing_times:\n",
        "        metrics['average_left_swing_time'] = np.mean(left_swing_times)\n",
        "    if right_swing_times:\n",
        "        metrics['average_right_swing_time'] = np.mean(right_swing_times)\n",
        "\n",
        "    # Calculate step and stride length estimates using the pixel_to_meter_ratio\n",
        "    # IMPORTANT: The pixel_to_meter_ratio needs to be accurately calibrated for meaningful results.\n",
        "    # See the comments in the Execution section for how to approach calibration.\n",
        "    if detected_events:\n",
        "         for event in detected_events:\n",
        "             if event.event_type == 'Heel Strike':\n",
        "                 if event.foot == 'Left' and current_landmarks is not None and len(current_landmarks) > RIGHT_ANKLE and len(current_landmarks) > LEFT_HEEL:\n",
        "                     # Estimate left step length (distance between left heel and right ankle at left heel strike)\n",
        "                     metrics['current_left_step_length_m'] = calculate_distance(current_landmarks[LEFT_HEEL], current_landmarks[RIGHT_ANKLE]) * pixel_to_meter_ratio\n",
        "                 elif event.foot == 'Right' and current_landmarks is not None and len(current_landmarks) > LEFT_ANKLE and len(current_landmarks) > RIGHT_HEEL:\n",
        "                     # Estimate right step length (distance between right heel and left ankle at right heel strike)\n",
        "                     metrics['current_right_step_length_m'] = calculate_distance(current_landmarks[RIGHT_HEEL], current_landmarks[LEFT_ANKLE]) * pixel_to_meter_ratio\n",
        "\n",
        "    # You can add more metric calculations here (e.g., cadence, double support time)\n",
        "\n",
        "    return metrics\n",
        "\n",
        "# --- Visualization Function ---\n",
        "def draw_stick_figure(image, landmarks, connections, visibility_th=0.5):\n",
        "    \"\"\"\n",
        "    Draws a stick figure on the image based on provided landmarks and connections.\n",
        "\n",
        "    Args:\n",
        "        image: The input image frame (NumPy array).\n",
        "        landmarks: A list of pose landmarks from MediaPipe.\n",
        "        connections: A list of tuples defining connections between landmarks.\n",
        "        visibility_th: Minimum visibility score for a landmark to be drawn.\n",
        "\n",
        "    Returns:\n",
        "        The image frame with the stick figure drawn.\n",
        "    \"\"\"\n",
        "    h, w, _ = image.shape\n",
        "    # Ensure landmarks are not None and have the expected structure\n",
        "    if landmarks is None:\n",
        "        return image\n",
        "    for connection in connections:\n",
        "        start_idx, end_idx = connection\n",
        "        # Use .value to get the integer index from the enum\n",
        "        start = landmarks[start_idx.value]\n",
        "        end = landmarks[end_idx.value]\n",
        "        if start.visibility > visibility_th and end.visibility > visibility_th:\n",
        "            x1, y1 = int(start.x * w), int(start.y * h)\n",
        "            x2, y2 = int(end.x * w), int(end.y * h)\n",
        "            cv2.line(image, (x1, y1), (x2, y2), (0, 255, 0), 2) # Draw in green\n",
        "    return image\n",
        "\n",
        "def visualize_gait(video_path_, output_path='gait_output.mp4'):\n",
        "    \"\"\"\n",
        "    Processes a video to draw a stick figure visualization of the pose.\n",
        "\n",
        "    Args:\n",
        "        video_path_: Path to the input video file.\n",
        "        output_path: Path to save the output video with visualization.\n",
        "    \"\"\"\n",
        "    print(f\"Attempting to open video for visualization: {video_path_}\") # Added print statement for debugging\n",
        "    cap = cv2.VideoCapture(video_path_)\n",
        "\n",
        "    # --- Added Error Handling for Video Capture ---\n",
        "    if not cap.isOpened():\n",
        "        print(f\"ERROR: Failed to open video file for visualization: {video_path_}. Please check the file path and ensure the video is not corrupted or in an unsupported format.\") # More specific error message\n",
        "        return False # Indicate failure\n",
        "\n",
        "    print(f\"Successfully opened video for visualization: {video_path_}\") # Added success message\n",
        "\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "    # Define the codec and create VideoWriter object\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v') # Codec for mp4\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "    # --- Added Error Handling for VideoWriter ---\n",
        "    if not out.isOpened():\n",
        "        print(f\"ERROR: Failed to initialize VideoWriter for output path: {output_path}. Ensure the codec ('mp4v') is supported and you have write permissions.\")\n",
        "        cap.release() # Release the video capture object\n",
        "        return False # Indicate failure\n",
        "\n",
        "    frame_count = 0\n",
        "    pose_detected_frames = 0\n",
        "\n",
        "    # Use a separate pose instance for visualization if needed, or reuse the main one\n",
        "    # with mp_pose.Pose(static_image_mode=False, model_complexity=1, enable_segmentation=False, min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose_viz:\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        results = pose.process(image_rgb) # Reuse the main pose instance\n",
        "\n",
        "        if results.pose_landmarks:\n",
        "            pose_detected_frames += 1\n",
        "            # Draw landmarks using mp_drawing.draw_landmarks (MediaPipe's default visualization)\n",
        "            mp_drawing.draw_landmarks(frame, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
        "                                      mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=2), # Orange dots\n",
        "                                      mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2)) # Pink connections\n",
        "            # Alternatively, use your custom draw_stick_figure function if preferred\n",
        "            # frame = draw_stick_figure(frame, results.pose_landmarks.landmark, POSE_CONNECTIONS)\n",
        "\n",
        "\n",
        "        out.write(frame) # Write the frame with visualization to the output video\n",
        "        frame_count += 1\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    print(f\"Visualization finished.\")\n",
        "    print(f\"Total frames processed for visualization: {frame_count}\")\n",
        "    print(f\"Frames with pose detected for visualization: {pose_detected_frames}\")\n",
        "    print(f\"Output visualization video saved to: {output_path}\")\n",
        "\n",
        "    return True # Indicate success\n",
        "\n",
        "\n",
        "# --- Main Pipeline Execution ---\n",
        "def run_gait_analysis_pipeline(video_path_):\n",
        "    \"\"\"\n",
        "    Runs the MediaPipe pose estimation and gait analysis pipeline on a video.\n",
        "\n",
        "    Args:\n",
        "        video_path_: Path to the input video file.\n",
        "    \"\"\"\n",
        "    print(f\"Attempting to open video for analysis: {video_path_}\") # Added print statement for debugging\n",
        "    cap = cv2.VideoCapture(video_path_)\n",
        "\n",
        "    # --- Added Error Handling for Video Capture ---\n",
        "    if not cap.isOpened():\n",
        "        print(f\"ERROR: Could not open video file for analysis: {video_path_}. Please check the file path and ensure the video is not corrupted or in an unsupported format.\") # More specific error message\n",
        "        return False # Indicate failure\n",
        "\n",
        "    print(f\"Successfully opened video for analysis: {video_path_}\") # Added success message\n",
        "\n",
        "\n",
        "    frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
        "    if frame_rate == 0:\n",
        "        print(\"Warning: Could not get frame rate from video. Assuming 30 FPS.\")\n",
        "        frame_rate = 30.0\n",
        "\n",
        "    # --- Pixel to Meter Ratio Calibration ---\n",
        "    # To get gait metrics in meters or centimeters, you need to calibrate the pixel_to_meter_ratio.\n",
        "    # This involves measuring a known real-world distance in the video frame and dividing it by the corresponding pixel distance.\n",
        "    # Replace the placeholder value below with your calculated ratio.\n",
        "    # Example: If a 1-meter ruler in the video is 500 pixels long, the ratio is 1 meter / 500 pixels = 0.002 meters/pixel.\n",
        "    # If you want centimeters, the ratio would be 100 cm / 500 pixels = 0.2 cm/pixel.\n",
        "    pixel_to_meter_ratio = 0.001 # Placeholder: Replace with your calibrated value!\n",
        "\n",
        "    # Initialize data structures for gait analysis\n",
        "    key_point_history = deque(maxlen=int(frame_rate * 5)) # Store history for 5 seconds for event detection\n",
        "    gait_cycles_left = [] # To store completed left gait cycles\n",
        "    gait_cycles_right = [] # To store completed right gait cycles\n",
        "\n",
        "    frame_count = 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    print(f\"Starting gait analysis for video: {video_path_}\")\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break # End of video\n",
        "\n",
        "        frame_count += 1\n",
        "        # Convert the BGR image to RGB for MediaPipe processing\n",
        "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        # Process the image and find pose landmarks.\n",
        "        results = pose.process(image)\n",
        "\n",
        "        # Get the current timestamp in microseconds\n",
        "        current_timestamp_us = int(cap.get(cv2.CAP_PROP_POS_MSEC) * 1000)\n",
        "\n",
        "        if results.pose_landmarks:\n",
        "            landmarks = results.pose_landmarks.landmark\n",
        "            # Store the full set of landmarks and timestamp in history\n",
        "            key_point_history.append((current_timestamp_us, landmarks))\n",
        "\n",
        "            # Detect gait events in the current frame using the history\n",
        "            detected_events = detect_gait_events(key_point_history, frame_rate)\n",
        "\n",
        "            # Calculate gait metrics based on the current state and detected events\n",
        "            # The calculated step/stride lengths will be in meters if pixel_to_meter_ratio is calibrated correctly in meters/pixel.\n",
        "            gait_metrics = calculate_gait_metrics(key_point_history, detected_events, gait_cycles_left, gait_cycles_right, pixel_to_meter_ratio)\n",
        "\n",
        "            # --- Output or Display Metrics ---\n",
        "            # Print the calculated metrics for the current frame/state\n",
        "            if gait_metrics:\n",
        "                print(f\"Frame: {frame_count}, Timestamp: {current_timestamp_us / 1000000.0:.2f}s, Metrics: {gait_metrics}\")\n",
        "\n",
        "        # In a Colab environment, cv2.imshow is not typically used for displaying video frames directly.\n",
        "        # The visualization is saved to a file.\n",
        "        # cv2.imshow('MediaPipe Pose and Gait Analysis', image)\n",
        "\n",
        "        # The waitKey and breaking the loop with 'q' is for local execution with GUI windows.\n",
        "        # if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        #     break\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"\\nGait analysis finished.\")\n",
        "    print(f\"Processed {frame_count} frames in {elapsed_time:.2f} seconds.\")\n",
        "    if elapsed_time > 0:\n",
        "        print(f\"Average FPS: {frame_count / elapsed_time:.2f}\")\n",
        "\n",
        "    cap.release()\n",
        "    # cv2.destroyAllWindows() # Not needed in Colab\n",
        "\n",
        "    return True # Indicate success\n",
        "\n",
        "\n",
        "# --- Execution ---\n",
        "# Ensure you have uploaded the video file to your Colab environment\n",
        "# and the path below correctly points to the uploaded file.\n",
        "# Use the files.upload() function in a separate cell to upload the video.\n",
        "# Example:\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "# video_path_ = list(uploaded.keys())[0] # Get the name of the uploaded file\n",
        "\n",
        "# Assuming the video is uploaded and named 'output_video_test_high_ (2).mp4' in the /content/ directory\n",
        "video_path_ = \"/content/output_video_test_high_ (2).mp4\"\n",
        "\n",
        "# Define the output path for the visualization video\n",
        "output_video_path = 'gait_stick_figure_output.mp4'\n",
        "\n",
        "# Check if the video file exists at the specified path\n",
        "if not os.path.exists(video_path_):\n",
        "    print(f\"Error: Video file not found at {video_path_}. Please upload the video and ensure the path is correct.\")\n",
        "else:\n",
        "    print(f\"Video file found at {video_path_}. Proceeding with analysis.\")\n",
        "    # Run the gait analysis pipeline to calculate and print metrics\n",
        "    # Remember to calibrate the pixel_to_meter_ratio in the run_gait_analysis_pipeline function\n",
        "    analysis_successful = run_gait_analysis_pipeline(video_path_)\n",
        "\n",
        "    # Run the visualization pipeline to generate the stick figure video\n",
        "    # The output video will be saved as 'gait_stick_figure_output.mp4' in the /content/ directory\n",
        "    if analysis_successful: # Only attempt visualization if analysis was successful\n",
        "        visualization_successful = visualize_gait(video_path_, output_path=output_video_path)\n",
        "\n",
        "        if visualization_successful:\n",
        "            print(\"\\nGait analysis and visualization complete.\")\n",
        "            print(f\"Attempting to download the output video '{output_video_path}'...\")\n",
        "            # --- Added automatic download of the output video ---\n",
        "            try:\n",
        "                files.download(output_video_path)\n",
        "                print(f\"Download initiated for '{output_video_path}'. Check your browser's downloads.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error initiating download for '{output_video_path}': {e}\")\n",
        "                print(f\"You can also try downloading the file manually from the file browser in Colab (look for '{output_video_path}' in the /content/ directory).\")\n",
        "        else:\n",
        "             print(\"\\nGait analysis complete, but visualization failed.\")\n",
        "    else:\n",
        "        print(\"\\nGait analysis failed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tTjV3jIe1Ak5",
        "outputId": "bd200475-a472-4027-a11e-a3f0be1df563"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video file found at /content/output_video_test_high_ (2).mp4. Proceeding with analysis.\n",
            "Attempting to open video for analysis: /content/output_video_test_high_ (2).mp4\n",
            "Successfully opened video for analysis: /content/output_video_test_high_ (2).mp4\n",
            "Starting gait analysis for video: /content/output_video_test_high_ (2).mp4\n",
            "Frame: 139, Timestamp: 2.30s, Metrics: {'current_left_step_length_m': 3.466096569658732e-05, 'current_right_step_length_m': 3.581988546268648e-05}\n",
            "Frame: 143, Timestamp: 2.37s, Metrics: {'current_left_step_length_m': 2.292943345945438e-05, 'current_right_step_length_m': 2.219481022785552e-05}\n",
            "Frame: 239, Timestamp: 3.97s, Metrics: {'average_right_stance_time': 1.599733, 'current_left_step_length_m': 0.00011429818836361981}\n",
            "Frame: 240, Timestamp: 3.98s, Metrics: {'average_left_stance_time': 0.016664000000000012, 'average_right_stance_time': 1.599733}\n",
            "Frame: 243, Timestamp: 4.03s, Metrics: {'average_left_cycle_time': 1.6663890000000001, 'average_right_cycle_time': 1.7330450000000002, 'average_left_stance_time': 0.016664000000000012, 'average_right_stance_time': 1.599733, 'average_left_swing_time': 0.049992000000000036, 'average_right_swing_time': 0.06665600000000005, 'current_left_step_length_m': 0.0002326286732214529, 'current_right_step_length_m': 0.00023719778212203405}\n",
            "Frame: 244, Timestamp: 4.05s, Metrics: {'average_left_cycle_time': 1.6663890000000001, 'average_right_cycle_time': 1.7330450000000002, 'average_left_stance_time': 0.016664000000000012, 'average_right_stance_time': 1.599733, 'average_left_swing_time': 0.049992000000000036, 'average_right_swing_time': 0.06665600000000005}\n",
            "Frame: 245, Timestamp: 4.07s, Metrics: {'average_right_cycle_time': 1.7330450000000002, 'average_left_stance_time': 0.016664000000000012, 'average_right_stance_time': 1.599733, 'average_left_swing_time': 0.049992000000000036, 'average_right_swing_time': 0.06665600000000005, 'current_left_step_length_m': 0.0001134857551425843}\n",
            "Frame: 246, Timestamp: 4.08s, Metrics: {'average_left_stance_time': 0.016664000000000012, 'average_right_stance_time': 1.599733, 'average_left_swing_time': 0.049992000000000036, 'average_right_swing_time': 0.06665600000000005, 'current_right_step_length_m': 0.00014470187258148825}\n",
            "Frame: 247, Timestamp: 4.10s, Metrics: {'average_left_stance_time': 0.016664000000000012, 'average_right_stance_time': 1.599733, 'average_left_swing_time': 0.049992000000000036, 'average_right_swing_time': 0.06665600000000005}\n",
            "Frame: 248, Timestamp: 4.12s, Metrics: {'average_right_stance_time': 1.599733, 'average_right_swing_time': 0.06665600000000005, 'current_left_step_length_m': 7.537811094778715e-05}\n",
            "Frame: 249, Timestamp: 4.13s, Metrics: {'average_right_stance_time': 1.599733, 'average_right_swing_time': 0.06665600000000005}\n",
            "Frame: 250, Timestamp: 4.15s, Metrics: {'average_right_stance_time': 1.599733, 'average_right_swing_time': 0.06665600000000005}\n",
            "Frame: 251, Timestamp: 4.17s, Metrics: {'average_right_stance_time': 1.599733, 'average_right_swing_time': 0.06665600000000005}\n",
            "Frame: 252, Timestamp: 4.18s, Metrics: {'average_right_stance_time': 1.599733, 'average_right_swing_time': 0.06665600000000005, 'current_left_step_length_m': 4.6500928231949555e-05}\n",
            "Frame: 253, Timestamp: 4.20s, Metrics: {'average_right_stance_time': 1.599733, 'average_right_swing_time': 0.06665600000000005}\n",
            "Frame: 254, Timestamp: 4.22s, Metrics: {'average_right_stance_time': 1.599733, 'average_right_swing_time': 0.06665600000000005}\n",
            "Frame: 255, Timestamp: 4.23s, Metrics: {'average_right_stance_time': 1.599733, 'average_right_swing_time': 0.06665600000000005}\n",
            "Frame: 256, Timestamp: 4.25s, Metrics: {'average_right_stance_time': 1.599733, 'average_right_swing_time': 0.06665600000000005}\n",
            "Frame: 257, Timestamp: 4.27s, Metrics: {'average_right_stance_time': 1.599733, 'average_right_swing_time': 0.06665600000000005}\n",
            "Frame: 258, Timestamp: 4.28s, Metrics: {'average_right_stance_time': 1.599733, 'average_right_swing_time': 0.06665600000000005}\n",
            "Frame: 259, Timestamp: 4.30s, Metrics: {'average_right_stance_time': 1.599733, 'average_right_swing_time': 0.06665600000000005}\n",
            "Frame: 260, Timestamp: 4.32s, Metrics: {'average_right_stance_time': 1.599733, 'average_right_swing_time': 0.06665600000000005}\n",
            "Frame: 261, Timestamp: 4.33s, Metrics: {'average_right_stance_time': 1.599733, 'average_right_swing_time': 0.06665600000000005}\n",
            "Frame: 262, Timestamp: 4.35s, Metrics: {'average_right_stance_time': 1.599733, 'average_right_swing_time': 0.06665600000000005}\n",
            "Frame: 263, Timestamp: 4.37s, Metrics: {'average_right_stance_time': 1.599733, 'average_right_swing_time': 0.06665600000000005}\n",
            "Frame: 264, Timestamp: 4.38s, Metrics: {'average_right_stance_time': 1.599733, 'average_right_swing_time': 0.06665600000000005}\n",
            "Frame: 265, Timestamp: 4.40s, Metrics: {'average_right_stance_time': 1.599733, 'average_right_swing_time': 0.06665600000000005}\n",
            "Frame: 266, Timestamp: 4.42s, Metrics: {'average_right_stance_time': 1.599733, 'average_right_swing_time': 0.06665600000000005}\n",
            "Frame: 267, Timestamp: 4.43s, Metrics: {'average_right_stance_time': 1.599733, 'average_right_swing_time': 0.06665600000000005}\n",
            "Frame: 268, Timestamp: 4.45s, Metrics: {'average_right_stance_time': 1.599733, 'average_right_swing_time': 0.06665600000000005}\n",
            "Frame: 269, Timestamp: 4.47s, Metrics: {'average_right_stance_time': 1.599733, 'average_right_swing_time': 0.06665600000000005}\n",
            "Frame: 270, Timestamp: 4.48s, Metrics: {'average_right_stance_time': 1.599733, 'average_right_swing_time': 0.06665600000000005}\n",
            "Frame: 271, Timestamp: 4.50s, Metrics: {'average_right_stance_time': 1.599733, 'average_right_swing_time': 0.06665600000000005}\n",
            "Frame: 272, Timestamp: 4.52s, Metrics: {'average_right_stance_time': 1.599733, 'average_right_swing_time': 0.06665600000000005}\n",
            "Frame: 273, Timestamp: 4.53s, Metrics: {'average_right_stance_time': 1.599733, 'average_right_swing_time': 0.06665600000000005}\n",
            "Frame: 274, Timestamp: 4.55s, Metrics: {'current_right_step_length_m': 8.137206474921535e-05}\n",
            "Frame: 275, Timestamp: 4.57s, Metrics: {'current_right_step_length_m': 5.3953928686218316e-05}\n",
            "Frame: 311, Timestamp: 5.17s, Metrics: {'current_left_step_length_m': 5.470830937261439e-05}\n",
            "Frame: 313, Timestamp: 5.20s, Metrics: {'current_left_step_length_m': 6.206992788150011e-05}\n",
            "Frame: 314, Timestamp: 5.22s, Metrics: {'current_left_step_length_m': 7.13256268917787e-05}\n",
            "Frame: 315, Timestamp: 5.23s, Metrics: {'current_left_step_length_m': 6.702137966604947e-05}\n",
            "Frame: 317, Timestamp: 5.27s, Metrics: {'current_left_step_length_m': 5.696701204123535e-05, 'current_right_step_length_m': 5.465335643357982e-05}\n",
            "Frame: 338, Timestamp: 5.62s, Metrics: {'current_right_step_length_m': 6.825713378783343e-05}\n",
            "Frame: 340, Timestamp: 5.65s, Metrics: {'current_right_step_length_m': 6.270562551036115e-05}\n",
            "Frame: 341, Timestamp: 5.67s, Metrics: {'current_right_step_length_m': 6.587378124500399e-05}\n",
            "Frame: 349, Timestamp: 5.80s, Metrics: {'current_right_step_length_m': 4.2627732893473276e-05}\n",
            "Frame: 350, Timestamp: 5.82s, Metrics: {'current_right_step_length_m': 4.351566733435892e-05}\n",
            "Frame: 359, Timestamp: 5.97s, Metrics: {'current_right_step_length_m': 7.09494231248545e-05}\n",
            "Frame: 372, Timestamp: 6.18s, Metrics: {'current_left_step_length_m': 6.517163326572174e-05}\n",
            "Frame: 373, Timestamp: 6.20s, Metrics: {'current_left_step_length_m': 6.980795528968766e-05}\n",
            "Frame: 374, Timestamp: 6.22s, Metrics: {'current_left_step_length_m': 7.475742151614687e-05}\n",
            "Frame: 375, Timestamp: 6.23s, Metrics: {'current_left_step_length_m': 7.816587177412833e-05}\n",
            "Frame: 378, Timestamp: 6.28s, Metrics: {'current_left_step_length_m': 9.669991995275908e-05}\n",
            "Frame: 379, Timestamp: 6.30s, Metrics: {'current_left_step_length_m': 0.00013116650372496525}\n",
            "Frame: 380, Timestamp: 6.32s, Metrics: {'current_left_step_length_m': 0.0001515083120938847}\n",
            "Frame: 395, Timestamp: 6.57s, Metrics: {'current_right_step_length_m': 9.070738500491786e-05}\n",
            "Frame: 398, Timestamp: 6.62s, Metrics: {'current_right_step_length_m': 6.741288501672935e-05}\n",
            "Frame: 400, Timestamp: 6.65s, Metrics: {'current_right_step_length_m': 7.329623140754201e-05}\n",
            "Frame: 408, Timestamp: 6.78s, Metrics: {'current_right_step_length_m': 3.9025708657672074e-05}\n",
            "Frame: 409, Timestamp: 6.80s, Metrics: {'current_right_step_length_m': 5.78047826488395e-05}\n",
            "Frame: 430, Timestamp: 7.15s, Metrics: {'current_left_step_length_m': 7.922696724868116e-05}\n",
            "Frame: 431, Timestamp: 7.17s, Metrics: {'current_left_step_length_m': 8.334888282254669e-05}\n",
            "Frame: 433, Timestamp: 7.20s, Metrics: {'current_left_step_length_m': 8.235454130934546e-05}\n",
            "Frame: 436, Timestamp: 7.25s, Metrics: {'current_left_step_length_m': 0.00016622575563385224}\n",
            "Frame: 437, Timestamp: 7.27s, Metrics: {'current_left_step_length_m': 0.0001412011640212806}\n",
            "Frame: 438, Timestamp: 7.28s, Metrics: {'current_left_step_length_m': 0.00020154987094512614}\n",
            "Frame: 458, Timestamp: 7.62s, Metrics: {'current_right_step_length_m': 0.00010632491897939622}\n",
            "Frame: 461, Timestamp: 7.67s, Metrics: {'current_right_step_length_m': 7.679281670183018e-05}\n",
            "Frame: 520, Timestamp: 8.65s, Metrics: {'current_left_step_length_m': 0.00018806112004912364}\n",
            "Frame: 521, Timestamp: 8.67s, Metrics: {'current_right_step_length_m': 0.00012174972067249533}\n",
            "Frame: 523, Timestamp: 8.70s, Metrics: {'current_left_step_length_m': 0.00013232917705641246}\n",
            "Frame: 526, Timestamp: 8.75s, Metrics: {'current_left_step_length_m': 0.0001812854891214503}\n",
            "\n",
            "Gait analysis finished.\n",
            "Processed 527 frames in 30.42 seconds.\n",
            "Average FPS: 17.32\n",
            "Attempting to open video for visualization: /content/output_video_test_high_ (2).mp4\n",
            "Successfully opened video for visualization: /content/output_video_test_high_ (2).mp4\n",
            "Visualization finished.\n",
            "Total frames processed for visualization: 527\n",
            "Frames with pose detected for visualization: 300\n",
            "Output visualization video saved to: gait_stick_figure_output.mp4\n",
            "\n",
            "Gait analysis and visualization complete.\n",
            "Attempting to download the output video 'gait_stick_figure_output.mp4'...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_24420a46-ceb1-40c3-92b5-415089cc558f\", \"gait_stick_figure_output.mp4\", 75515134)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download initiated for 'gait_stick_figure_output.mp4'. Check your browser's downloads.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "video_path = \"/content/output_video_test_high_ (2).mp4\"\n",
        "visualize_gait(video_path_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "laQPr03fg69H",
        "outputId": "f762e33e-b2b5-4081-e495-c8b43fde0ee9"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total frames processed: 527\n",
            "Frames with pose detected: 300\n",
            "Output video saved to: gait_output.mp4\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}